{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KAERI_source_code.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Nj76qUe2lnA"
      },
      "source": [
        "## [DACON] 진동데이터 활용 충돌체 탐지 AI 경진대회\n",
        "## 데이터 잘 모르는 사람 (팀명) (Team name)\n",
        "## 2020년 7월 17일 (제출날짜) (Submission date)\n",
        "\n",
        "### Part1 CNN Model\n",
        "### Part2 Wavenet\n",
        "### Part3 ensembles and post process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYHb_Mf-2lnG"
      },
      "source": [
        "# Part1 CNN Model\n",
        "## 1. 라이브러리 및 데이터\n",
        "## Library & Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtKnyRor2lnI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "d0f87293-22a2-4b5b-9fda-3f64937eb83b"
      },
      "source": [
        "import math\n",
        "\n",
        "from sklearn.model_selection import KFold, StratifiedKFold, GroupKFold\n",
        "import lightgbm as lgb\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from tqdm.notebook import tqdm\n",
        "import gc\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import warnings\n",
        "from scipy.signal import find_peaks\n",
        "import os\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "import os\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, MaxPooling1D, Conv1D, Conv2D, Flatten,MaxPooling2D,BatchNormalization,Lambda, AveragePooling2D\n",
        "import keras.backend as K\n",
        "from keras.models import load_model\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "weight1 = np.array([1,1,0,0])\n",
        "weight2 = np.array([0,0,1,1])\n",
        "def my_loss(y_true, y_pred):\n",
        "    divResult = Lambda(lambda x: x[0]/x[1])([(y_pred-y_true),(y_true+0.000001)])\n",
        "    return K.mean(K.square(divResult))\n",
        "def my_loss_E1(y_true, y_pred):\n",
        "    return K.mean(K.square(y_true-y_pred)*weight1)/2e+04\n",
        "def my_loss_E2(y_true, y_pred):\n",
        "    divResult = Lambda(lambda x: x[0]/x[1])([(y_pred-y_true),(y_true+0.000001)])\n",
        "    return K.mean(K.square(divResult)*weight2)\n",
        "def kaeri_metric(y_true, y_pred):\n",
        "    return 0.5 * E1(y_true, y_pred) + 0.5 * E2(y_true, y_pred)\n",
        "def E1(y_true, y_pred):\n",
        "    _t, _p = np.array(y_true)[:,:2], np.array(y_pred)[:,:2]\n",
        "    return np.mean(np.sum(np.square(_t - _p), axis = 1) / 2e+04)\n",
        "def E2(y_true, y_pred):\n",
        "    _t, _p = np.array(y_true)[:,2:], np.array(y_pred)[:,2:]\n",
        "    return np.mean(np.sum(np.square((_t - _p) / (_t + 1e-06)), axis = 1))\n",
        "def E2M(y_true, y_pred):\n",
        "    _t, _p = np.array(y_true)[:,2], np.array(y_pred)[:,2]\n",
        "    return np.mean(np.square((_t - _p) / (_t + 1e-06)))\n",
        "def E2V(y_true, y_pred):\n",
        "    _t, _p = np.array(y_true)[:,3], np.array(y_pred)[:,3]\n",
        "    return np.mean(np.square((_t - _p) / (_t + 1e-06)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n",
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWmCSqg83xsx"
      },
      "source": [
        "def read_data(window_sizes=[[], [], []]):\n",
        "    train_df = pd.read_csv('../data/train_features.csv')\n",
        "    target_df = pd.read_csv('../data/train_target.csv')\n",
        "    test_df = pd.read_csv('../data/test_features.csv')\n",
        "    train_df = train_df[train_df.index%375<thres].reset_index(drop=True)\n",
        "    test_df = test_df[test_df.index%375<thres].reset_index(drop=True)\n",
        "\n",
        "    X_train_l, X_test_l = [], []\n",
        "    Y_train = target_df.values[:, 1:]\n",
        "\n",
        "    # making XY\n",
        "    train = train_df.copy()\n",
        "    test = test_df.copy()\n",
        "    for col in ['S1', 'S2', 'S3', 'S4']:\n",
        "        train[f'speed_{col}'] = train.groupby('id')[col].cumsum().abs().values\n",
        "        test[f'speed_{col}'] = test.groupby('id')[col].cumsum().abs().values\n",
        "    train, test = normalize(train, test)\n",
        "    train = lag_with_pct_change0(train, window_sizes[0])\n",
        "    test = lag_with_pct_change0(test, window_sizes[0])\n",
        "\n",
        "    X_train = train.values.reshape((2800,thres,-1,1))\n",
        "    X_test = test.values.reshape((700,thres,-1,1))\n",
        "    X_train_l += [X_train]\n",
        "    X_test_l += [X_test]\n",
        "\n",
        "    # making M\n",
        "    train = train_df.copy()\n",
        "    test = test_df.copy()\n",
        "    for col in ['S1', 'S2', 'S3', 'S4']:\n",
        "        train[f'speed_{col}'] = train.groupby('id')[col].cumsum().abs().values\n",
        "        test[f'speed_{col}'] = test.groupby('id')[col].cumsum().abs().values\n",
        "    train, test = normalize(train, test)\n",
        "    train = lag_with_pct_change1(train, window_sizes[1])\n",
        "    test = lag_with_pct_change1(test, window_sizes[1])\n",
        "\n",
        "    X_train = train.values.reshape((2800,thres,-1,1))\n",
        "    X_test = test.values.reshape((700,thres,-1,1))\n",
        "    X_train_l += [X_train]\n",
        "    X_test_l += [X_test]\n",
        "\n",
        "    # making V\n",
        "    train = train_df.copy()\n",
        "    test = test_df.copy()\n",
        "\n",
        "    train, test = normalize(train, test)\n",
        "    train = lag_with_pct_change2(train, window_sizes[2])\n",
        "    test = lag_with_pct_change2(test, window_sizes[2])\n",
        "    \n",
        "    X_train = train.values.reshape((2800,thres,-1,1))\n",
        "    X_test = test.values.reshape((700,thres,-1,1))\n",
        "    X_train_l += [X_train]\n",
        "    X_test_l += [X_test]\n",
        "\n",
        "    for i in range(3):\n",
        "        if i==0:\n",
        "            print(f'Y_train.shape : {Y_train.shape}')\n",
        "        print(f'train shape {i} : {X_train_l[i].shape}, test shape {i} : {X_test_l[i].shape}')\n",
        "    return X_train_l, Y_train, X_test_l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zjQY_KY2lnR"
      },
      "source": [
        "## 2. 데이터 전처리\n",
        "## Data Cleansing & Pre-Processing  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pb0OD3v82lnT"
      },
      "source": [
        "def lag_with_pct_change0(df, windows=[]):\n",
        "    for window in [1, 2, 3]:\n",
        "        for col in ['S1', 'S2', 'S3', 'S4']:\n",
        "            df['signal_shift_pos_' + str(window) + col] = df.groupby('id')[col].shift(window).fillna(0)\n",
        "            df['signal_shift_neg_' + str(window) + col] = df.groupby('id')[col].shift(-1 * window).fillna(0)\n",
        "    for window in windows:\n",
        "        for col in ['S1', 'S2', 'S3', 'S4']:\n",
        "            df[f'{col}_{window}_mean'] = df.groupby('id')[col].rolling(window=window).mean().values\n",
        "            df[f'{col}_{window}_std'] = df.groupby('id')[col].rolling(window=window).std().values\n",
        "            df[f'{col}_{window}_var'] = df.groupby('id')[col].rolling(window=window).var().values\n",
        "            df[f'{col}_{window}_max'] = df.groupby('id')[col].rolling(window=window).max().values\n",
        "            df[f'{col}_{window}_min'] = df.groupby('id')[col].rolling(window=window).min().values\n",
        "    df.drop(columns='id', inplace=True)\n",
        "    df = df.replace([np.inf, -np.inf], np.nan)\n",
        "    df.fillna(0, inplace=True)\n",
        "    return df\n",
        "##############################################################################################################################\n",
        "def lag_with_pct_change1(df, windows=[]):\n",
        "    for window in [1, 2, 3]:\n",
        "        for col in ['S1', 'S2', 'S3', 'S4']:\n",
        "            df['signal_shift_pos_' + str(window) + col] = df.groupby('id')[col].shift(window).fillna(0)\n",
        "            df['signal_shift_neg_' + str(window) + col] = df.groupby('id')[col].shift(-1 * window).fillna(0)\n",
        "        for col in [f'speed_{c}' for c in ['S1', 'S2', 'S3', 'S4']]:\n",
        "            df['signal_shift_pos_' + str(window) + col] = df.groupby('id')[col].shift(window).fillna(0)\n",
        "            df['signal_shift_neg_' + str(window) + col] = df.groupby('id')[col].shift(-1 * window).fillna(0)\n",
        "    for window in windows:\n",
        "        for col in ['S1', 'S2', 'S3', 'S4']:\n",
        "            df[f'{col}_{window}_mean'] = df.groupby('id')[col].rolling(window=window).mean().values\n",
        "            df[f'{col}_{window}_std'] = df.groupby('id')[col].rolling(window=window).std().values\n",
        "            df[f'{col}_{window}_var'] = df.groupby('id')[col].rolling(window=window).var().values\n",
        "            df[f'{col}_{window}_max'] = df.groupby('id')[col].rolling(window=window).max().values\n",
        "            df[f'{col}_{window}_min'] = df.groupby('id')[col].rolling(window=window).min().values\n",
        "    df.drop(columns='id', inplace=True)\n",
        "    df = df.replace([np.inf, -np.inf], np.nan)\n",
        "    df.fillna(0, inplace=True)\n",
        "    return df\n",
        "##############################################################################################################################\n",
        "def lag_with_pct_change2(df, windows=[]):\n",
        "    for window in windows:\n",
        "        for col in ['S1', 'S2', 'S3', 'S4']:\n",
        "            df[f'{col}_{window}_mean'] = df.groupby('id')[col].rolling(window=window).mean().values\n",
        "            df[f'{col}_{window}_max'] = df.groupby('id')[col].rolling(window=window).max().values\n",
        "            df[f'{col}_{window}_min'] = df.groupby('id')[col].rolling(window=window).min().values\n",
        "    df.drop(columns=['id'], inplace=True)\n",
        "    df = df.replace([np.inf, -np.inf], np.nan)\n",
        "    df.fillna(0, inplace=True)\n",
        "    return df\n",
        "##############################################################################################################################\n",
        "def normalize(train, test):\n",
        "    if 'S1' in train.columns:\n",
        "        for col in ['Time', 'S1', 'S2', 'S3', 'S4']:\n",
        "            train_input_mean = train[col].mean()\n",
        "            train_input_sigma = train[col].std()\n",
        "            train[col] = (train[col] - train_input_mean) / train_input_sigma\n",
        "            test[col] = (test[col] - train_input_mean) / train_input_sigma\n",
        "    if 'speed_S1' in train.columns:\n",
        "        for col in ['speed_S1', 'speed_S2', 'speed_S3', 'speed_S4']:\n",
        "            train_input_mean = train[col].mean()\n",
        "            train_input_sigma = train[col].std()\n",
        "            train[col] = (train[col] - train_input_mean) / train_input_sigma\n",
        "            test[col] = (test[col] - train_input_mean) / train_input_sigma\n",
        "    return train, test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwLtCHGC2lnb"
      },
      "source": [
        "## 3. 탐색적 자료분석\n",
        "## Exploratory Data Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qyq90ZzB2lnk"
      },
      "source": [
        "## 4. 변수 선택 및 모델 구축\n",
        "## Feature Engineering & Initial Modeling  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rel1AkRP2lnm"
      },
      "source": [
        "def set_model(train_target, nf=16):  # 0:x,y, 1:m, 2:v\n",
        "    activation = 'elu'\n",
        "    padding = 'valid'\n",
        "    model = Sequential()\n",
        "    \n",
        "    fs = (3,1)\n",
        "\n",
        "    model.add(Conv2D(nf,fs, padding=padding, activation=activation, input_shape=X_train_l[train_target].shape[1:]))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 1)))\n",
        "\n",
        "    model.add(Conv2D(nf*2,fs, padding=padding, activation=activation))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 1)))\n",
        "\n",
        "    model.add(Conv2D(nf*4,fs, padding=padding, activation=activation))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 1)))\n",
        "\n",
        "    model.add(Conv2D(nf*8,fs, padding=padding, activation=activation))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 1)))\n",
        "\n",
        "    model.add(Conv2D(nf*16,fs, padding=padding, activation=activation))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 1)))\n",
        "\n",
        "    model.add(Conv2D(nf*32,fs, padding=padding, activation=activation))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 1)))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation ='elu'))\n",
        "    model.add(Dense(64, activation ='elu'))\n",
        "    model.add(Dense(32, activation ='elu'))\n",
        "    model.add(Dense(16, activation ='elu'))\n",
        "    model.add(Dense(4))\n",
        "\n",
        "    optimizer = keras.optimizers.Adam()\n",
        "\n",
        "    global weight2\n",
        "    if train_target == 1: # only for M\n",
        "        weight2 = np.array([0,0,1,0])\n",
        "    else: # only for V\n",
        "        weight2 = np.array([0,0,0,1])\n",
        "       \n",
        "    if train_target==0:\n",
        "        model.compile(loss=my_loss_E1, optimizer=optimizer,)\n",
        "    else:\n",
        "        model.compile(loss=my_loss_E2,optimizer=optimizer,)\n",
        "       \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V21KUtyl2lnu"
      },
      "source": [
        "## 5. 모델 학습 및 검증\n",
        "## Model Tuning & Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H77sph0aUHT-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "6245a000-6d76-4f4d-d4f9-9bbe3de202df"
      },
      "source": [
        "%%time\n",
        "thres=200\n",
        "N_FOLDS=5\n",
        "MODEL_SAVE_FOLDER_PATH = ''\n",
        "\n",
        "SEED=42\n",
        "\n",
        "ws = [[30], [10, 50], [30]]\n",
        "X_train_l, Y_train, X_test_l = read_data(window_sizes=ws)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Y_train.shape : (2800, 4)\n",
            "train shape 0 : (2800, 200, 53, 1), test shape 0 : (700, 200, 53, 1)\n",
            "train shape 1 : (2800, 200, 97, 1), test shape 1 : (700, 200, 97, 1)\n",
            "train shape 2 : (2800, 200, 17, 1), test shape 2 : (700, 200, 17, 1)\n",
            "CPU times: user 2min 26s, sys: 1.03 s, total: 2min 27s\n",
            "Wall time: 2min 36s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNU-ahud2lnv"
      },
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "def plot_error(type_id,pred,true):\n",
        "    print(pred.shape)\n",
        "\n",
        "    if type_id == 0:\n",
        "        _name = 'x_pos'\n",
        "    elif type_id == 1:\n",
        "        _name = 'y_pos'\n",
        "    elif type_id == 2:\n",
        "        _name = 'mass'\n",
        "    elif type_id == 3:\n",
        "        _name = 'velocity'\n",
        "    elif type_id == 4:\n",
        "        _name = \"distance\"\n",
        "    else:\n",
        "        _name = 'error'\n",
        "\n",
        "    x_coord = np.arange(1,pred.shape[0]+1,1)\n",
        "    if type_id < 2:\n",
        "        Err_m = (pred[:,type_id] - true[:,type_id])\n",
        "    elif type_id < 4:\n",
        "        Err_m = ((pred[:,type_id] - true[:,type_id])/true[:,type_id])*100\n",
        "    else:\n",
        "        Err_m = ((pred[:,0]-true[:,0])**2+(pred[:,1]-true[:,1])**2)**0.5\n",
        "\n",
        "    fig = plt.figure(figsize=(8,6))\n",
        "    plt.rcParams[\"font.size\"]=15\n",
        "    plt.scatter(x_coord, Err_m, marker='o')\n",
        "    plt.title(\"%s Prediction for Training Data\" % _name, size=20)\n",
        "    plt.xlabel(\"Data ID\", labelpad=10, size=20)\n",
        "    plt.ylabel(\"Prediction Error of %s,\" % _name, labelpad=10, size=20)\n",
        "    plt.xticks(size=15)\n",
        "    plt.yticks(size=15)\n",
        "    plt.ylim(-100., 100.)\n",
        "    plt.xlim(0, pred.shape[0]+1)\n",
        "    plt.show()\n",
        "    print('std : {}, max : {}, min : {}'.format(np.std(Err_m), np.max(Err_m), np.min(Err_m)))\n",
        "    return Err_m\n",
        "\n",
        "def train(model,X,Y,train_target,n_fold=5,fold=0,seed=42):\n",
        "    best_save = ModelCheckpoint(MODEL_SAVE_FOLDER_PATH + f'best_m{train_target}_{fold}.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
        "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=100)\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=10, min_lr=0.00001, verbose=1)\n",
        "\n",
        "    kf = KFold(n_splits=n_fold, random_state=seed, shuffle=True)\n",
        "    splits = [[i,j] for i,j in kf.split(X)]\n",
        "    tt, tt_target = X[splits[fold][0]], Y[splits[fold][0]]\n",
        "    vv, vv_target = X[splits[fold][1]], Y[splits[fold][1]]\n",
        "    \n",
        "    EPOCHS=500 if train_target==2 else 300\n",
        "    history = model.fit(tt, tt_target, validation_data=(vv, vv_target),\n",
        "                  epochs=EPOCHS,\n",
        "                  batch_size=512,\n",
        "                  shuffle=True,\n",
        "                  verbose = 2,\n",
        "                  callbacks=[es, best_save, reduce_lr], )\n",
        "\n",
        "    fig, loss_ax = plt.subplots()\n",
        "    acc_ax = loss_ax.twinx()\n",
        "\n",
        "    loss_ax.plot(history.history['loss'], 'y', label='train loss')\n",
        "    loss_ax.plot(history.history['val_loss'], 'r', label='val loss')\n",
        "    loss_ax.axvline(x=np.argmin(history.history['val_loss']), color='b', linewidth=1)\n",
        "    loss_ax.set_xlabel('epoch')\n",
        "    loss_ax.set_ylabel('loss')\n",
        "    loss_ax.legend(loc='upper left')\n",
        "    plt.show()    \n",
        "    \n",
        "    return model\n",
        "def load_best_model(train_target, fold=0):\n",
        "    if train_target == 0:\n",
        "        model = load_model(MODEL_SAVE_FOLDER_PATH + f'best_m{train_target}_{fold}.hdf5', custom_objects={'my_loss_E1': my_loss, })\n",
        "    else:\n",
        "        model = load_model(MODEL_SAVE_FOLDER_PATH + f'best_m{train_target}_{fold}.hdf5', custom_objects={'my_loss_E2': my_loss, })\n",
        "\n",
        "    score = model.evaluate(X_train_l[train_target], Y_train, verbose=0)\n",
        "    print('loss:', score)\n",
        "\n",
        "    pred = model.predict(X_train_l[train_target])\n",
        "    i=0\n",
        "    print(f'정답(original): {Y_train[i]}, 예측값(original):, {pred[i]}')\n",
        "    print(f'E1 : {E1(Y_train, pred)}, E2 : {E2(Y_train, pred)}, E2M : {E2M(Y_train, pred)}, E2v : {E2V(Y_train, pred)}')\n",
        "    \n",
        "    if train_target ==0:\n",
        "        plot_error(4,pred,Y_train)\n",
        "    elif train_target ==1:\n",
        "        plot_error(2,pred,Y_train)\n",
        "    elif train_target ==2:\n",
        "        plot_error(3,pred,Y_train)    \n",
        "    \n",
        "    return model, [E1(Y_train, pred), E2(Y_train, pred), E2M(Y_train, pred), E2V(Y_train, pred)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMp8Yzmp4QWK"
      },
      "source": [
        "if 'submit' not in globals(): submit=pd.read_csv('../data/sample_submission.csv')\n",
        "if 'metrics' not in globals(): metrics = {}\n",
        "\n",
        "seed_everything(SEED)\n",
        "nf_list = [16, 16, 16]\n",
        "for train_target in range(3):\n",
        "    fold_metrics = {}\n",
        "    for k in range(N_FOLDS):\n",
        "        model = set_model(train_target, nf=nf_list[train_target])\n",
        "        train(model, X_train_l[train_target], Y_train, train_target, n_fold=N_FOLDS, fold=k, seed=SEED)\n",
        "        \n",
        "        best_model, fold_metrics[k] = load_best_model(train_target, k)\n",
        "        pred_data_test = best_model.predict(X_test_l[train_target])\n",
        "\n",
        "        if k==0:\n",
        "            if train_target == 0: # x,y 학습\n",
        "                submit.iloc[:,1] = pred_data_test[:,0]/N_FOLDS\n",
        "                submit.iloc[:,2] = pred_data_test[:,1]/N_FOLDS\n",
        "            elif train_target == 1: # m 학습\n",
        "                submit.iloc[:,3] = pred_data_test[:,2]/N_FOLDS\n",
        "            elif train_target == 2: # v 학습\n",
        "                submit.iloc[:,4] = pred_data_test[:,3]/N_FOLDS\n",
        "        else:\n",
        "            if train_target == 0: # x,y 학습\n",
        "                submit.iloc[:,1] += pred_data_test[:,0]/N_FOLDS\n",
        "                submit.iloc[:,2] += pred_data_test[:,1]/N_FOLDS\n",
        "            elif train_target == 1: # m 학습\n",
        "                submit.iloc[:,3] += pred_data_test[:,2]/N_FOLDS\n",
        "            elif train_target == 2: # v 학습\n",
        "                submit.iloc[:,4] += pred_data_test[:,3]/N_FOLDS\n",
        "        print(f'################################################################################ target : {train_target}, fold : {k} ################################################################################')\n",
        "    metrics[train_target] = fold_metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ru1CcWdBWmRV"
      },
      "source": [
        "cnn_submit = submit.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbceDtH7WmUv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NULl2-iqWmYe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2mGzoE1v8QO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sRAZwqV8OJr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFs9ZKxu81at"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbFzcvkY81eB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SwkYl6o81hD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6hCwnH081kP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wexdXwc081nc"
      },
      "source": [
        "# Part2 Wavenet\n",
        "## 1. 라이브러리 및 데이터\n",
        "## Library & Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAJ_pc2T4ntu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "119f1d1b-45fe-475c-b5f3-4c34632d93a8"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import math\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import KFold, StratifiedKFold, GroupKFold\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "!pip install torchcontrib\n",
        "from torchcontrib.optim import SWA\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from tqdm.notebook import tqdm\n",
        "import gc\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import pairwise_distances\n",
        "# from lookagead import Lookahead\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', 1000)\n",
        "pd.set_option('display.max_rows', 500)\n",
        "\n",
        "import os\n",
        "from keras import backend as K\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.svm import SVR, NuSVR\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "# Any results you write to the current directory are saved as output.\n",
        "\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchcontrib in /usr/local/lib/python3.6/dist-packages (0.0.2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQICfiN2Z-bw"
      },
      "source": [
        "## 2. 데이터 전처리\n",
        "## Data Cleansing & Pre-Processing  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V68Ljkd04nxb"
      },
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    # torch.backends.cudnn.deterministic = True\n",
        "\n",
        "def read_data(debar=375):\n",
        "    train = pd.read_csv('../data/train_features.csv')\n",
        "    target = pd.read_csv('../data/train_target.csv')\n",
        "    test = pd.read_csv('../data/test_features.csv')\n",
        "    sub = pd.read_csv('../data/sample_submission.csv')\n",
        "    \n",
        "    train = train[train.index%375<debar].reset_index(drop=True)\n",
        "    test = test[test.index%375<debar].reset_index(drop=True)\n",
        "\n",
        "    for col in ['S1', 'S2', 'S3', 'S4']:\n",
        "        train[f'speed_{col}'] = train.groupby('id')[col].cumsum().abs().values\n",
        "        test[f'speed_{col}'] = test.groupby('id')[col].cumsum().abs().values\n",
        "    return train, target, test, sub\n",
        "\n",
        "def normalize(train, test):\n",
        "    for col in ['S1', 'S2', 'S3', 'S4']:\n",
        "        train_input_mean = train[col].mean()\n",
        "        train_input_sigma = train[col].std()\n",
        "        train[col] = (train[col] - train_input_mean) / train_input_sigma\n",
        "        test[col] = (test[col] - train_input_mean) / train_input_sigma\n",
        "\n",
        "        col = f'speed_{col}'\n",
        "        train_input_mean = train[col].mean()\n",
        "        train_input_sigma = train[col].std()\n",
        "        train[col] = (train[col] - train_input_mean) / train_input_sigma\n",
        "        test[col] = (test[col] - train_input_mean) / train_input_sigma\n",
        "    return train, test\n",
        "\n",
        "def lag_with_pct_change(df, windows):\n",
        "    for window in [1, 2, 3]:\n",
        "        for col in ['S1', 'S2', 'S3', 'S4']:\n",
        "            df['signal_shift_pos_' + str(window) + col] = df.groupby('id')[col].shift(window).fillna(0)\n",
        "            df['signal_shift_neg_' + str(window) + col] = df.groupby('id')[col].shift(-1 * window).fillna(0)\n",
        "\n",
        "    for window in windows:\n",
        "        for col in ['S1', 'S2', 'S3', 'S4']:\n",
        "            df[f'{col}_{window}_mean'] = df.groupby('id')[col].rolling(window=window).mean().values\n",
        "            df[f'{col}_{window}_std'] = df.groupby('id')[col].rolling(window=window).std().values\n",
        "            df[f'{col}_{window}_var'] = df.groupby('id')[col].rolling(window=window).var().values\n",
        "            df[f'{col}_{window}_max'] = df.groupby('id')[col].rolling(window=window).max().values\n",
        "            df[f'{col}_{window}_min'] = df.groupby('id')[col].rolling(window=window).min().values\n",
        "\n",
        "        for col in ['S1', 'S2', 'S3', 'S4']:\n",
        "            col = f'speed_{col}'\n",
        "            df[f'{col}_{window}_mean'] = df.groupby('id')[col].rolling(window=window).mean().values\n",
        "            df[f'{col}_{window}_std'] = df.groupby('id')[col].rolling(window=window).std().values\n",
        "            df[f'{col}_{window}_var'] = df.groupby('id')[col].rolling(window=window).var().values\n",
        "            df[f'{col}_{window}_max'] = df.groupby('id')[col].rolling(window=window).max().values\n",
        "            df[f'{col}_{window}_min'] = df.groupby('id')[col].rolling(window=window).min().values\n",
        "\n",
        "    for col in ['S1', 'S2', 'S3', 'S4']:\n",
        "        df.drop(columns=col, inplace=True)\n",
        "        df.drop(columns=f'speed_{col}', inplace=True)\n",
        "            \n",
        "    df = df.replace([np.inf, -np.inf], np.nan)    \n",
        "    df.fillna(0, inplace=True)\n",
        "    return df\n",
        "\n",
        "def run_feat_engineering(df, batch_size):\n",
        "    df = lag_with_pct_change(df, [10, 50])\n",
        "    return df\n",
        "\n",
        "# fillna with the mean and select features for training\n",
        "def feature_selection(train, test):\n",
        "    features = [col for col in train.columns if col not in ['index', 'id']]#Time\n",
        "    train = train.replace([np.inf, -np.inf], np.nan)\n",
        "    test = test.replace([np.inf, -np.inf], np.nan)\n",
        "    for feature in features:\n",
        "        # 나중에 group별로 fillna 하는 걸로 바꿔보자\n",
        "        feature_mean = pd.concat([train[feature], test[feature]], axis = 0).mean()\n",
        "        train[feature] = train[feature].fillna(feature_mean)\n",
        "        test[feature] = test[feature].fillna(feature_mean)\n",
        "    return train, test, features\n",
        "\n",
        "def split(GROUP_BATCH_SIZE=4000, SPLITS=5):\n",
        "    print('Reading Data Started...')\n",
        "    train, target, test, sub = read_data(debar=GROUP_BATCH_SIZE)\n",
        "    train, test = normalize(train, test)\n",
        "    print('Reading and Normalizing Data Completed')\n",
        "    print('Creating Features')\n",
        "    print('Feature Engineering Started...')\n",
        "    train = run_feat_engineering(train, batch_size=GROUP_BATCH_SIZE)\n",
        "    test = run_feat_engineering(test, batch_size=GROUP_BATCH_SIZE)\n",
        "    \n",
        "    train, test, features = feature_selection(train, test)\n",
        "    print('Feature Engineering Completed...')\n",
        "    print(features)\n",
        "\n",
        "    train = pd.merge(train, target, how='left', on='id')\n",
        "    group = train['id']\n",
        "    kf = GroupKFold(n_splits=SPLITS)\n",
        "    splits = [x for x in kf.split(train, train[['X', 'Y', 'M', 'V']], group)]\n",
        "    \n",
        "    new_splits = []\n",
        "    for sp in splits:\n",
        "        new_split = []\n",
        "        new_split.append(np.unique(group[sp[0]]))\n",
        "        new_split.append(np.unique(group[sp[1]]))\n",
        "        new_split.append(sp[1])\n",
        "        new_splits.append(new_split)\n",
        "    target_cols = ['X', 'Y', 'M', 'V']\n",
        "    \n",
        "    target = np.array(list(train.groupby('id').apply(lambda x: x[target_cols].values))).astype(np.float32)\n",
        "    train = np.array(list(train.groupby('id').apply(lambda x: x[features].values)))\n",
        "    \n",
        "    test = np.array(list(test.groupby('id').apply(lambda x: x[features].values)))\n",
        "\n",
        "    print(train.shape, test.shape, target.shape)\n",
        "    return train, test, target, new_splits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2g5Odiu4aD6N"
      },
      "source": [
        "## 3. 탐색적 자료분석\n",
        "## Exploratory Data Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yYvIJqhaD6O"
      },
      "source": [
        "## 4. 변수 선택 및 모델 구축\n",
        "## Feature Engineering & Initial Modeling  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfd8HZJg4wnI"
      },
      "source": [
        "class Wave_Block(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,dilation_rates):\n",
        "        super(Wave_Block,self).__init__()\n",
        "        self.num_rates = dilation_rates\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.filter_convs = nn.ModuleList()\n",
        "        self.gate_convs = nn.ModuleList()\n",
        "        self.convs.append(nn.Conv1d(in_channels,out_channels,kernel_size=1))\n",
        "        dilation_rates = [2**i for i in range(dilation_rates)]\n",
        "        for dilation_rate in dilation_rates:\n",
        "            self.filter_convs.append(nn.Conv1d(out_channels,out_channels,kernel_size=3,padding=dilation_rate,dilation=dilation_rate))\n",
        "            self.gate_convs.append(nn.Conv1d(out_channels,out_channels,kernel_size=3,padding=dilation_rate,dilation=dilation_rate))\n",
        "            self.convs.append(nn.Conv1d(out_channels,out_channels,kernel_size=1))\n",
        "    def forward(self,x):\n",
        "        x = self.convs[0](x)\n",
        "        res = x\n",
        "        for i in range(self.num_rates):\n",
        "            x = F.tanh(self.filter_convs[i](x))*F.sigmoid(self.gate_convs[i](x))\n",
        "            x = self.convs[i+1](x)\n",
        "            res = torch.add(res, x)\n",
        "        return res\n",
        "\n",
        "class Regressor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "        # For normal input\n",
        "        self.wave_block1 = Wave_Block(105,64,12)\n",
        "        self.bn_1 = nn.BatchNorm1d(64)\n",
        "\n",
        "        self.wave_block2 = Wave_Block(64,32,8)\n",
        "        self.bn_2 = nn.BatchNorm1d(32)\n",
        "\n",
        "        self.wave_block3 = Wave_Block(32,64,4)\n",
        "        self.bn_3 = nn.BatchNorm1d(64)\n",
        "\n",
        "        self.wave_block4 = Wave_Block(64,128,1)\n",
        "        self.bn_4 = nn.BatchNorm1d(128)\n",
        "\n",
        "        self.fc = nn.Linear(128, 4)\n",
        "\n",
        "    def flip(self, x, dim):\n",
        "        dim = x.dim() + dim if dim < 0 else dim\n",
        "        return x[tuple(slice(None, None) if i != dim\n",
        "                 else torch.arange(x.size(i)-1, -1, -1).long()\n",
        "                 for i in range(x.dim()))]\n",
        "    \n",
        "    def forward(self,x):\n",
        "        x = x.permute(0, 2, 1)\n",
        "        # forward input\n",
        "        x = self.wave_block1(x)\n",
        "        x = self.bn_1(x)\n",
        "        x = self.wave_block2(x)\n",
        "        x = self.bn_2(x)\n",
        "        x = self.wave_block3(x)\n",
        "        x = self.bn_3(x)\n",
        "        x = self.wave_block4(x)\n",
        "        x = self.bn_4(x)\n",
        "\n",
        "        x = torch.mean(x, axis=2)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "    \n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, delta=0, checkpoint_path='checkpoint.pt', is_maximize=True):\n",
        "        self.patience, self.delta, self.checkpoint_path = patience, delta, checkpoint_path\n",
        "        self.counter, self.best_score = 0, None\n",
        "        self.is_maximize = is_maximize\n",
        "    def load_best_weights(self, model):\n",
        "        model.load_state_dict(torch.load(self.checkpoint_path))\n",
        "    def __call__(self, score, model):\n",
        "        if self.best_score is None or \\\n",
        "                (score > self.best_score + self.delta if self.is_maximize else score < self.best_score - self.delta):\n",
        "            torch.save(model.state_dict(), self.checkpoint_path)\n",
        "            self.best_score, self.counter = score, 0\n",
        "            return 1\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                return 2\n",
        "        return 0\n",
        "\n",
        "class IronDataset(Dataset):\n",
        "    def __init__(self, data, labels, training=True, transform=False, seq_len=5000, flip=False, noise_level=0, class_split=0.0):\n",
        "        \n",
        "        if flip:\n",
        "            self.data = np.flip(data)\n",
        "            self.labels = np.flip(labels)\n",
        "        else:\n",
        "            self.data = data\n",
        "            self.labels = labels.mean(1)\n",
        "        \n",
        "        self.transform = transform\n",
        "        self.training = training\n",
        "        self.flip = flip\n",
        "        self.noise_level = noise_level\n",
        "        self.class_split = class_split\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        data = self.data[idx]\n",
        "        labels = self.labels[idx]\n",
        "        return [data.astype(np.float32), labels.astype(np.float32)]\n",
        "\n",
        "def kaeri_metric(y_true, y_pred):\n",
        "    return 0.5 * E1(y_true, y_pred) + 0.5 * E2(y_true, y_pred)\n",
        "def E1(y_true, y_pred):\n",
        "    _t, _p = np.array(y_true)[:,:2], np.array(y_pred)[:,:2]\n",
        "    return np.mean(np.sum(np.square(_t - _p), axis = 1) / 2e+04)\n",
        "def E2(y_true, y_pred):\n",
        "    _t, _p = np.array(y_true)[:,2:], np.array(y_pred)[:,2:]\n",
        "    return np.mean(np.sum(np.square((_t - _p) / (_t + 1e-06)), axis = 1))\n",
        "\n",
        "def torch_kaeri_metric(y_true, y_pred):\n",
        "    return 0.5 * torch_E1(y_true, y_pred) + 0.5 * torch_E2(y_true, y_pred)\n",
        "def torch_E1(y_true, y_pred):\n",
        "    _t, _p = y_true[:,:2], y_pred[:,:2]\n",
        "    return torch.mean(torch.sum(torch.square(_t - _p), axis = 1) / 2e+04)\n",
        "def torch_E2(y_true, y_pred):\n",
        "    _t, _p = y_true[:,2:], y_pred[:,2:]\n",
        "    return torch.mean(torch.sum(torch.square((_t - _p) / (_t + 1e-06)), axis = 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNzeTHx2aGrv"
      },
      "source": [
        "## 5. 모델 학습 및 검증\n",
        "## Model Tuning & Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7K9dASN4wrE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "90ee2810-0af8-44eb-e24d-fcee863e5750"
      },
      "source": [
        "%%time\n",
        "GROUP_BATCH_SIZE=200\n",
        "train, test, train_tr, new_splits = split(GROUP_BATCH_SIZE=GROUP_BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading Data Started...\n",
            "Reading and Normalizing Data Completed\n",
            "Creating Features\n",
            "Feature Engineering Started...\n",
            "Feature Engineering Completed...\n",
            "['Time', 'signal_shift_pos_1S1', 'signal_shift_neg_1S1', 'signal_shift_pos_1S2', 'signal_shift_neg_1S2', 'signal_shift_pos_1S3', 'signal_shift_neg_1S3', 'signal_shift_pos_1S4', 'signal_shift_neg_1S4', 'signal_shift_pos_2S1', 'signal_shift_neg_2S1', 'signal_shift_pos_2S2', 'signal_shift_neg_2S2', 'signal_shift_pos_2S3', 'signal_shift_neg_2S3', 'signal_shift_pos_2S4', 'signal_shift_neg_2S4', 'signal_shift_pos_3S1', 'signal_shift_neg_3S1', 'signal_shift_pos_3S2', 'signal_shift_neg_3S2', 'signal_shift_pos_3S3', 'signal_shift_neg_3S3', 'signal_shift_pos_3S4', 'signal_shift_neg_3S4', 'S1_10_mean', 'S1_10_std', 'S1_10_var', 'S1_10_max', 'S1_10_min', 'S2_10_mean', 'S2_10_std', 'S2_10_var', 'S2_10_max', 'S2_10_min', 'S3_10_mean', 'S3_10_std', 'S3_10_var', 'S3_10_max', 'S3_10_min', 'S4_10_mean', 'S4_10_std', 'S4_10_var', 'S4_10_max', 'S4_10_min', 'speed_S1_10_mean', 'speed_S1_10_std', 'speed_S1_10_var', 'speed_S1_10_max', 'speed_S1_10_min', 'speed_S2_10_mean', 'speed_S2_10_std', 'speed_S2_10_var', 'speed_S2_10_max', 'speed_S2_10_min', 'speed_S3_10_mean', 'speed_S3_10_std', 'speed_S3_10_var', 'speed_S3_10_max', 'speed_S3_10_min', 'speed_S4_10_mean', 'speed_S4_10_std', 'speed_S4_10_var', 'speed_S4_10_max', 'speed_S4_10_min', 'S1_50_mean', 'S1_50_std', 'S1_50_var', 'S1_50_max', 'S1_50_min', 'S2_50_mean', 'S2_50_std', 'S2_50_var', 'S2_50_max', 'S2_50_min', 'S3_50_mean', 'S3_50_std', 'S3_50_var', 'S3_50_max', 'S3_50_min', 'S4_50_mean', 'S4_50_std', 'S4_50_var', 'S4_50_max', 'S4_50_min', 'speed_S1_50_mean', 'speed_S1_50_std', 'speed_S1_50_var', 'speed_S1_50_max', 'speed_S1_50_min', 'speed_S2_50_mean', 'speed_S2_50_std', 'speed_S2_50_var', 'speed_S2_50_max', 'speed_S2_50_min', 'speed_S3_50_mean', 'speed_S3_50_std', 'speed_S3_50_var', 'speed_S3_50_max', 'speed_S3_50_min', 'speed_S4_50_mean', 'speed_S4_50_std', 'speed_S4_50_var', 'speed_S4_50_max', 'speed_S4_50_min']\n",
            "(2800, 200, 105) (700, 200, 105) (2800, 200, 4)\n",
            "CPU times: user 2min 50s, sys: 1.38 s, total: 2min 52s\n",
            "Wall time: 2min 52s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDn1ZETX4wt_"
      },
      "source": [
        "NNBATCHSIZE = 256\n",
        "flip = False\n",
        "noise = False\n",
        "LR = 0.0015\n",
        "EPOCHS = 200\n",
        "outdir = ''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q391TQOo4wxP"
      },
      "source": [
        "seed_everything(42)\n",
        "test_y = np.zeros([int(262500/GROUP_BATCH_SIZE), GROUP_BATCH_SIZE, 4])\n",
        "test_dataset = IronDataset(test, test_y, flip=flip)\n",
        "test_dataloader = DataLoader(test_dataset, NNBATCHSIZE, shuffle=False, num_workers=8, pin_memory=True)\n",
        "\n",
        "for index, (train_index, val_index, _) in enumerate(new_splits[0:], start=0):\n",
        "    print(\"Fold : {}\".format(index))\n",
        "    train_dataset = IronDataset(train[train_index], train_tr[train_index], seq_len=GROUP_BATCH_SIZE, flip=flip, noise_level=False, transform=False)\n",
        "    train_dataloader = DataLoader(train_dataset, NNBATCHSIZE, shuffle=True, num_workers=8, pin_memory=True)\n",
        "\n",
        "    valid_dataset = IronDataset(train[val_index], train_tr[val_index], seq_len=GROUP_BATCH_SIZE, flip=flip)\n",
        "    valid_dataloader = DataLoader(valid_dataset, NNBATCHSIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "    model = Regressor()\n",
        "    model = model.cuda()\n",
        "\n",
        "    early_stopping = EarlyStopping(patience=50, is_maximize=False, checkpoint_path=os.path.join(outdir, f'fold_{index}.pt'))\n",
        "    \n",
        "    opt = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=0.000025)\n",
        "    schedular = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', patience=5, factor=0.9)\n",
        "\n",
        "    criterion = nn.L1Loss()\n",
        "    score = 10000000\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        train_loss, valid_loss = 0, 0\n",
        "        custom_trn_loss, custom_val_loss = 0, 0\n",
        "\n",
        "        model.train()  # prep model for training\n",
        "\n",
        "        for (x, y) in train_dataloader:\n",
        "            x = x.cuda()\n",
        "            y = y.cuda()\n",
        "            \n",
        "            opt.zero_grad()\n",
        "            predictions = model(x)\n",
        "            loss = criterion(predictions, y)\n",
        "            custom = torch_kaeri_metric(y, predictions)\n",
        "            loss = loss + custom*2\n",
        "\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            custom_trn_loss += custom\n",
        "        custom_trn_loss = custom_trn_loss / len(train_dataloader)\n",
        "\n",
        "        # prep model for evaluation\n",
        "        model.eval()  \n",
        "        with torch.no_grad():\n",
        "            for (x, y) in valid_dataloader:\n",
        "                x = x.cuda()\n",
        "                y = y.cuda()\n",
        "                \n",
        "                predictions = model(x)\n",
        "                loss = criterion(predictions, y)\n",
        "                custom = torch_kaeri_metric(y, predictions).item()\n",
        "                \n",
        "                valid_loss += loss\n",
        "                custom_val_loss += custom\n",
        "        \n",
        "        valid_loss = valid_loss / len(valid_dataloader)\n",
        "        custom_val_loss = custom_val_loss / len(valid_dataloader)\n",
        "        \n",
        "        early_loss = valid_loss + custom_val_loss*2\n",
        "        res = early_stopping(early_loss, model)\n",
        "        \n",
        "        if res==2:\n",
        "            print(\"Early Stopping\")\n",
        "            print('folder %d global best val max f1 model score %f' % (index, early_stopping.best_score))\n",
        "            print (\"Epoch [{}/{}], val_loss : {:.4f}, tr_cus_loss : {:.4f}, va_cus_loss : {:.4f}, lr : {:.6f}, early_loss : {:.4f}\".\n",
        "                    format(epoch+1, EPOCHS, valid_loss, custom_trn_loss, custom_val_loss, opt.param_groups[0]['lr'], early_loss))\n",
        "            break\n",
        "        elif res==1:\n",
        "            if score>early_loss:\n",
        "                score = early_loss\n",
        "                print (\"Epoch [{}/{}], val_loss : {:.4f}, tr_cus_loss : {:.4f}, va_cus_loss : {:.4f}, lr : {:.6f}, early_loss : {:.4f}\".\n",
        "                    format(epoch+1, EPOCHS, valid_loss, custom_trn_loss, custom_val_loss, opt.param_groups[0]['lr'], early_loss))\n",
        "        \n",
        "        # 플레쳐\n",
        "        schedular.step(early_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "www8POOD45Xr"
      },
      "source": [
        "seed_everything(42)\n",
        "n_folds = 5\n",
        "test_y = np.zeros([700, 4])\n",
        "test_dataset = IronDataset(test, test_y, flip=flip)\n",
        "test_dataloader = DataLoader(test_dataset, NNBATCHSIZE, shuffle=False, num_workers=8, pin_memory=True)\n",
        "\n",
        "oof_mae = 0\n",
        "oof_custom = 0\n",
        "\n",
        "for index, (train_index, val_index, _) in enumerate(new_splits[0:], start=0):\n",
        "    print(\"Fold : {}\".format(index))\n",
        "    valid_dataset = IronDataset(train[val_index], train_tr[val_index], seq_len=GROUP_BATCH_SIZE, flip=flip)\n",
        "    valid_dataloader = DataLoader(valid_dataset, NNBATCHSIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "    model = Regressor()\n",
        "    model = model.cuda()\n",
        "    model.load_state_dict(torch.load(outdir + f'fold_{index}.pt'))\n",
        "\n",
        "    model.eval()  # prep model for evaluation\n",
        "    oof = []\n",
        "    with torch.no_grad():\n",
        "        for (x, y) in valid_dataloader:\n",
        "            x = x.cuda()\n",
        "            y = y.cuda()\n",
        "            \n",
        "            pred = model(x)\n",
        "            pred = pred.detach().cpu().numpy()\n",
        "            oof += [pred]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (x, y) in enumerate(test_dataloader):\n",
        "            x = x.cuda()\n",
        "            y = y.cuda()\n",
        "\n",
        "            predictions = model(x)\n",
        "            predictions = predictions.detach().cpu().numpy()\n",
        "\n",
        "            test_y[NNBATCHSIZE*i:NNBATCHSIZE*(i+1), :] += predictions / n_folds\n",
        "\n",
        "    valid_oof = np.concatenate(oof)\n",
        "    valid_ture = train_tr[val_index].mean(1)\n",
        "    print('mae : {:.6f}, custom_metirc : {:.6f}'.format(mean_absolute_error(valid_ture, valid_oof), kaeri_metric(valid_ture, valid_oof)))\n",
        "    \n",
        "    oof_mae += mean_absolute_error(valid_ture, valid_oof) / n_folds\n",
        "    oof_custom += kaeri_metric(valid_ture, valid_oof) / n_folds\n",
        "print(oof_mae, oof_custom)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B831iCCA469B"
      },
      "source": [
        "submit.iloc[:, 1:] = test_y\n",
        "wavenet_submit = submit.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7W8BWh7YTQX2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiL17yuHf_QN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eR_xIGYhf_Th"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzhQQUqJf_Wa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVKWSgQ1f_Zf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fp3GrjrFf_cm"
      },
      "source": [
        "# Part3 CNN Model Pseudo\n",
        "## 1. 라이브러리 및 데이터\n",
        "## Library & Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRnyNnGOf_cn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "81eed5cb-417b-407f-d108-3b53cd10bdd2"
      },
      "source": [
        "import math\n",
        "\n",
        "from sklearn.model_selection import KFold, StratifiedKFold, GroupKFold\n",
        "import lightgbm as lgb\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from tqdm.notebook import tqdm\n",
        "import gc\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import warnings\n",
        "from scipy.signal import find_peaks\n",
        "import os\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "import os\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, MaxPooling1D, Conv1D, Conv2D, Flatten,MaxPooling2D,BatchNormalization,Lambda, AveragePooling2D\n",
        "import keras.backend as K\n",
        "from keras.models import load_model\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "weight1 = np.array([1,1,0,0])\n",
        "weight2 = np.array([0,0,1,1])\n",
        "def my_loss(y_true, y_pred):\n",
        "    divResult = Lambda(lambda x: x[0]/x[1])([(y_pred-y_true),(y_true+0.000001)])\n",
        "    return K.mean(K.square(divResult))\n",
        "def my_loss_E1(y_true, y_pred):\n",
        "    return K.mean(K.square(y_true-y_pred)*weight1)/2e+04\n",
        "def my_loss_E2(y_true, y_pred):\n",
        "    divResult = Lambda(lambda x: x[0]/x[1])([(y_pred-y_true),(y_true+0.000001)])\n",
        "    return K.mean(K.square(divResult)*weight2)\n",
        "def kaeri_metric(y_true, y_pred):\n",
        "    return 0.5 * E1(y_true, y_pred) + 0.5 * E2(y_true, y_pred)\n",
        "def E1(y_true, y_pred):\n",
        "    _t, _p = np.array(y_true)[:,:2], np.array(y_pred)[:,:2]\n",
        "    return np.mean(np.sum(np.square(_t - _p), axis = 1) / 2e+04)\n",
        "def E2(y_true, y_pred):\n",
        "    _t, _p = np.array(y_true)[:,2:], np.array(y_pred)[:,2:]\n",
        "    return np.mean(np.sum(np.square((_t - _p) / (_t + 1e-06)), axis = 1))\n",
        "def E2M(y_true, y_pred):\n",
        "    _t, _p = np.array(y_true)[:,2], np.array(y_pred)[:,2]\n",
        "    return np.mean(np.square((_t - _p) / (_t + 1e-06)))\n",
        "def E2V(y_true, y_pred):\n",
        "    _t, _p = np.array(y_true)[:,3], np.array(y_pred)[:,3]\n",
        "    return np.mean(np.square((_t - _p) / (_t + 1e-06)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n",
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4sSI77Gf_cq"
      },
      "source": [
        "def read_data(window_sizes=[[], [], []]):\n",
        "    train_df = pd.read_csv('../data/train_features.csv')\n",
        "    target_df = pd.read_csv('../data/train_target.csv')\n",
        "    test_df = pd.read_csv('../data/test_features.csv')\n",
        "    train_df = train_df[train_df.index%375<thres].reset_index(drop=True)\n",
        "    test_df = test_df[test_df.index%375<thres].reset_index(drop=True)\n",
        "\n",
        "    X_train_l, X_test_l = [], []\n",
        "    Y_train = target_df.values[:, 1:]\n",
        "\n",
        "    # making XY\n",
        "    train = train_df.copy()\n",
        "    test = test_df.copy()\n",
        "    for col in ['S1', 'S2', 'S3', 'S4']:\n",
        "        train[f'speed_{col}'] = train.groupby('id')[col].cumsum().abs().values\n",
        "        test[f'speed_{col}'] = test.groupby('id')[col].cumsum().abs().values\n",
        "    train, test = normalize(train, test)\n",
        "    train = lag_with_pct_change0(train, window_sizes[0])\n",
        "    test = lag_with_pct_change0(test, window_sizes[0])\n",
        "\n",
        "    X_train = train.values.reshape((2800,thres,-1,1))\n",
        "    X_test = test.values.reshape((700,thres,-1,1))\n",
        "    X_train_l += [X_train]\n",
        "    X_test_l += [X_test]\n",
        "\n",
        "    # making M\n",
        "    train = train_df.copy()\n",
        "    test = test_df.copy()\n",
        "    for col in ['S1', 'S2', 'S3', 'S4']:\n",
        "        train[f'speed_{col}'] = train.groupby('id')[col].cumsum().abs().values\n",
        "        test[f'speed_{col}'] = test.groupby('id')[col].cumsum().abs().values\n",
        "    train, test = normalize(train, test)\n",
        "    train = lag_with_pct_change1(train, window_sizes[1])\n",
        "    test = lag_with_pct_change1(test, window_sizes[1])\n",
        "\n",
        "    X_train = train.values.reshape((2800,thres,-1,1))\n",
        "    X_test = test.values.reshape((700,thres,-1,1))\n",
        "    X_train_l += [X_train]\n",
        "    X_test_l += [X_test]\n",
        "\n",
        "    # making V\n",
        "    train = train_df.copy()\n",
        "    test = test_df.copy()\n",
        "\n",
        "    train, test = normalize(train, test)\n",
        "    train = lag_with_pct_change2(train, window_sizes[2])\n",
        "    test = lag_with_pct_change2(test, window_sizes[2])\n",
        "    \n",
        "    X_train = train.values.reshape((2800,thres,-1,1))\n",
        "    X_test = test.values.reshape((700,thres,-1,1))\n",
        "    X_train_l += [X_train]\n",
        "    X_test_l += [X_test]\n",
        "\n",
        "    for i in range(3):\n",
        "        if i==0:\n",
        "            print(f'Y_train.shape : {Y_train.shape}')\n",
        "        print(f'train shape {i} : {X_train_l[i].shape}, test shape {i} : {X_test_l[i].shape}')\n",
        "    return X_train_l, Y_train, X_test_l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_z5AhElef_ct"
      },
      "source": [
        "## 2. 데이터 전처리\n",
        "## Data Cleansing & Pre-Processing  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkRL-Scef_ct"
      },
      "source": [
        "def lag_with_pct_change0(df, windows=[]):\n",
        "    for window in [1, 2, 3]:\n",
        "        for col in ['S1', 'S2', 'S3', 'S4']:\n",
        "            df['signal_shift_pos_' + str(window) + col] = df.groupby('id')[col].shift(window).fillna(0)\n",
        "            df['signal_shift_neg_' + str(window) + col] = df.groupby('id')[col].shift(-1 * window).fillna(0)\n",
        "    for window in windows:\n",
        "        for col in ['S1', 'S2', 'S3', 'S4']:\n",
        "            df[f'{col}_{window}_mean'] = df.groupby('id')[col].rolling(window=window).mean().values\n",
        "            df[f'{col}_{window}_std'] = df.groupby('id')[col].rolling(window=window).std().values\n",
        "            df[f'{col}_{window}_var'] = df.groupby('id')[col].rolling(window=window).var().values\n",
        "            df[f'{col}_{window}_max'] = df.groupby('id')[col].rolling(window=window).max().values\n",
        "            df[f'{col}_{window}_min'] = df.groupby('id')[col].rolling(window=window).min().values\n",
        "    df.drop(columns='id', inplace=True)\n",
        "    df = df.replace([np.inf, -np.inf], np.nan)\n",
        "    df.fillna(0, inplace=True)\n",
        "    return df\n",
        "##############################################################################################################################\n",
        "def lag_with_pct_change1(df, windows=[]):\n",
        "    for window in [1, 2, 3]:\n",
        "        for col in ['S1', 'S2', 'S3', 'S4']:\n",
        "            df['signal_shift_pos_' + str(window) + col] = df.groupby('id')[col].shift(window).fillna(0)\n",
        "            df['signal_shift_neg_' + str(window) + col] = df.groupby('id')[col].shift(-1 * window).fillna(0)\n",
        "        for col in [f'speed_{c}' for c in ['S1', 'S2', 'S3', 'S4']]:\n",
        "            df['signal_shift_pos_' + str(window) + col] = df.groupby('id')[col].shift(window).fillna(0)\n",
        "            df['signal_shift_neg_' + str(window) + col] = df.groupby('id')[col].shift(-1 * window).fillna(0)\n",
        "    for window in windows:\n",
        "        for col in ['S1', 'S2', 'S3', 'S4']:\n",
        "            df[f'{col}_{window}_mean'] = df.groupby('id')[col].rolling(window=window).mean().values\n",
        "            df[f'{col}_{window}_std'] = df.groupby('id')[col].rolling(window=window).std().values\n",
        "            df[f'{col}_{window}_var'] = df.groupby('id')[col].rolling(window=window).var().values\n",
        "            df[f'{col}_{window}_max'] = df.groupby('id')[col].rolling(window=window).max().values\n",
        "            df[f'{col}_{window}_min'] = df.groupby('id')[col].rolling(window=window).min().values\n",
        "    df.drop(columns='id', inplace=True)\n",
        "    df = df.replace([np.inf, -np.inf], np.nan)\n",
        "    df.fillna(0, inplace=True)\n",
        "    return df\n",
        "##############################################################################################################################\n",
        "def lag_with_pct_change2(df, windows=[]):\n",
        "    for window in windows:\n",
        "        for col in ['S1', 'S2', 'S3', 'S4']:\n",
        "            df[f'{col}_{window}_mean'] = df.groupby('id')[col].rolling(window=window).mean().values\n",
        "            df[f'{col}_{window}_max'] = df.groupby('id')[col].rolling(window=window).max().values\n",
        "            df[f'{col}_{window}_min'] = df.groupby('id')[col].rolling(window=window).min().values\n",
        "    df.drop(columns=['id'], inplace=True)\n",
        "    df = df.replace([np.inf, -np.inf], np.nan)\n",
        "    df.fillna(0, inplace=True)\n",
        "    return df\n",
        "##############################################################################################################################\n",
        "def normalize(train, test):\n",
        "    if 'S1' in train.columns:\n",
        "        for col in ['Time', 'S1', 'S2', 'S3', 'S4']:\n",
        "            train_input_mean = train[col].mean()\n",
        "            train_input_sigma = train[col].std()\n",
        "            train[col] = (train[col] - train_input_mean) / train_input_sigma\n",
        "            test[col] = (test[col] - train_input_mean) / train_input_sigma\n",
        "    if 'speed_S1' in train.columns:\n",
        "        for col in ['speed_S1', 'speed_S2', 'speed_S3', 'speed_S4']:\n",
        "            train_input_mean = train[col].mean()\n",
        "            train_input_sigma = train[col].std()\n",
        "            train[col] = (train[col] - train_input_mean) / train_input_sigma\n",
        "            test[col] = (test[col] - train_input_mean) / train_input_sigma\n",
        "    return train, test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynG3vM6Tf_cw"
      },
      "source": [
        "## 3. 탐색적 자료분석\n",
        "## Exploratory Data Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3deaYWCxf_cw"
      },
      "source": [
        "## 4. 변수 선택 및 모델 구축\n",
        "## Feature Engineering & Initial Modeling  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEknpfjef_cx"
      },
      "source": [
        "def set_model(train_target, nf=16):  # 0:x,y, 1:m, 2:v\n",
        "    activation = 'elu'\n",
        "    padding = 'valid'\n",
        "    model = Sequential()\n",
        "    \n",
        "    fs = (3,1)\n",
        "\n",
        "    model.add(Conv2D(nf,fs, padding=padding, activation=activation, input_shape=X_train_l[train_target].shape[1:]))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 1)))\n",
        "\n",
        "    model.add(Conv2D(nf*2,fs, padding=padding, activation=activation))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 1)))\n",
        "\n",
        "    model.add(Conv2D(nf*4,fs, padding=padding, activation=activation))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 1)))\n",
        "\n",
        "    model.add(Conv2D(nf*8,fs, padding=padding, activation=activation))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 1)))\n",
        "\n",
        "    model.add(Conv2D(nf*16,fs, padding=padding, activation=activation))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 1)))\n",
        "\n",
        "    model.add(Conv2D(nf*32,fs, padding=padding, activation=activation))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 1)))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation ='elu'))\n",
        "    model.add(Dense(64, activation ='elu'))\n",
        "    model.add(Dense(32, activation ='elu'))\n",
        "    model.add(Dense(16, activation ='elu'))\n",
        "    model.add(Dense(4))\n",
        "\n",
        "    optimizer = keras.optimizers.Adam()\n",
        "\n",
        "    global weight2\n",
        "    if train_target == 1: # only for M\n",
        "        weight2 = np.array([0,0,1,0])\n",
        "    else: # only for V\n",
        "        weight2 = np.array([0,0,0,1])\n",
        "       \n",
        "    if train_target==0:\n",
        "        model.compile(loss=my_loss_E1, optimizer=optimizer,)\n",
        "    else:\n",
        "        model.compile(loss=my_loss_E2,optimizer=optimizer,)\n",
        "       \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aa8aEDT9f_cz"
      },
      "source": [
        "## 5. 모델 학습 및 검증\n",
        "## Model Tuning & Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMSJLX_tf_cz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "6c548de7-448e-4fb6-adf2-155c0d122cea"
      },
      "source": [
        "%%time\n",
        "thres=200\n",
        "N_FOLDS=5\n",
        "MODEL_SAVE_FOLDER_PATH = ''\n",
        "\n",
        "SEED=42\n",
        "\n",
        "ws = [[30], [10, 50], [30]]\n",
        "X_train_l, Y_train, X_test_l = read_data(window_sizes=ws)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Y_train.shape : (2800, 4)\n",
            "train shape 0 : (2800, 200, 53, 1), test shape 0 : (700, 200, 53, 1)\n",
            "train shape 1 : (2800, 200, 97, 1), test shape 1 : (700, 200, 97, 1)\n",
            "train shape 2 : (2800, 200, 17, 1), test shape 2 : (700, 200, 17, 1)\n",
            "CPU times: user 3min 31s, sys: 1.25 s, total: 3min 32s\n",
            "Wall time: 3min 36s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNL6f0Eff_c3"
      },
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "def plot_error(type_id,pred,true):\n",
        "    print(pred.shape)\n",
        "\n",
        "    if type_id == 0:\n",
        "        _name = 'x_pos'\n",
        "    elif type_id == 1:\n",
        "        _name = 'y_pos'\n",
        "    elif type_id == 2:\n",
        "        _name = 'mass'\n",
        "    elif type_id == 3:\n",
        "        _name = 'velocity'\n",
        "    elif type_id == 4:\n",
        "        _name = \"distance\"\n",
        "    else:\n",
        "        _name = 'error'\n",
        "\n",
        "    x_coord = np.arange(1,pred.shape[0]+1,1)\n",
        "    if type_id < 2:\n",
        "        Err_m = (pred[:,type_id] - true[:,type_id])\n",
        "    elif type_id < 4:\n",
        "        Err_m = ((pred[:,type_id] - true[:,type_id])/true[:,type_id])*100\n",
        "    else:\n",
        "        Err_m = ((pred[:,0]-true[:,0])**2+(pred[:,1]-true[:,1])**2)**0.5\n",
        "\n",
        "    fig = plt.figure(figsize=(8,6))\n",
        "    plt.rcParams[\"font.size\"]=15\n",
        "    plt.scatter(x_coord, Err_m, marker='o')\n",
        "    plt.title(\"%s Prediction for Training Data\" % _name, size=20)\n",
        "    plt.xlabel(\"Data ID\", labelpad=10, size=20)\n",
        "    plt.ylabel(\"Prediction Error of %s,\" % _name, labelpad=10, size=20)\n",
        "    plt.xticks(size=15)\n",
        "    plt.yticks(size=15)\n",
        "    plt.ylim(-100., 100.)\n",
        "    plt.xlim(0, pred.shape[0]+1)\n",
        "    plt.show()\n",
        "    print('std : {}, max : {}, min : {}'.format(np.std(Err_m), np.max(Err_m), np.min(Err_m)))\n",
        "    return Err_m\n",
        "\n",
        "def train(model,X,Y,train_target,n_fold=5,fold=0,seed=42):\n",
        "    best_save = ModelCheckpoint(MODEL_SAVE_FOLDER_PATH + f'best_m{train_target}_{fold}.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
        "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=100)\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=10, min_lr=0.00001, verbose=1)\n",
        "\n",
        "    kf = KFold(n_splits=n_fold, random_state=seed, shuffle=True)\n",
        "    splits = [[i,j] for i,j in kf.split(X)]\n",
        "    tt, tt_target = X[splits[fold][0]], Y[splits[fold][0]]\n",
        "    vv, vv_target = X[splits[fold][1]], Y[splits[fold][1]]\n",
        "    \n",
        "    # pseudo labeling\n",
        "    splits2 = [[i,j] for i,j in kf.split(X_test_l[train_target])]\n",
        "    tt = np.concatenate([tt, X_test_l[train_target][splits2[fold][1]]])\n",
        "    tt_target = np.concatenate([tt_target, temp_target[splits2[fold][1]]])\n",
        "\n",
        "    EPOCHS=500 if train_target==2 else 300\n",
        "    history = model.fit(tt, tt_target, validation_data=(vv, vv_target),\n",
        "                  epochs=EPOCHS,\n",
        "                  batch_size=512,\n",
        "                  shuffle=True,\n",
        "                  verbose = 2,\n",
        "                  callbacks=[es, best_save, reduce_lr], )\n",
        "\n",
        "    fig, loss_ax = plt.subplots()\n",
        "    acc_ax = loss_ax.twinx()\n",
        "\n",
        "    loss_ax.plot(history.history['loss'], 'y', label='train loss')\n",
        "    loss_ax.plot(history.history['val_loss'], 'r', label='val loss')\n",
        "    loss_ax.axvline(x=np.argmin(history.history['val_loss']), color='b', linewidth=1)\n",
        "    loss_ax.set_xlabel('epoch')\n",
        "    loss_ax.set_ylabel('loss')\n",
        "    loss_ax.legend(loc='upper left')\n",
        "    plt.show()    \n",
        "    \n",
        "    return model\n",
        "def load_best_model(train_target, fold=0):\n",
        "    if train_target == 0:\n",
        "        model = load_model(MODEL_SAVE_FOLDER_PATH + f'best_m{train_target}_{fold}.hdf5', custom_objects={'my_loss_E1': my_loss, })\n",
        "    else:\n",
        "        model = load_model(MODEL_SAVE_FOLDER_PATH + f'best_m{train_target}_{fold}.hdf5', custom_objects={'my_loss_E2': my_loss, })\n",
        "\n",
        "    score = model.evaluate(X_train_l[train_target], Y_train, verbose=0)\n",
        "    print('loss:', score)\n",
        "\n",
        "    pred = model.predict(X_train_l[train_target])\n",
        "    i=0\n",
        "    print(f'정답(original): {Y_train[i]}, 예측값(original):, {pred[i]}')\n",
        "    print(f'E1 : {E1(Y_train, pred)}, E2 : {E2(Y_train, pred)}, E2M : {E2M(Y_train, pred)}, E2v : {E2V(Y_train, pred)}')\n",
        "    \n",
        "    if train_target ==0:\n",
        "        plot_error(4,pred,Y_train)\n",
        "    elif train_target ==1:\n",
        "        plot_error(2,pred,Y_train)\n",
        "    elif train_target ==2:\n",
        "        plot_error(3,pred,Y_train)    \n",
        "    \n",
        "    return model, [E1(Y_train, pred), E2(Y_train, pred), E2M(Y_train, pred), E2V(Y_train, pred)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKkOxXjgR7_S"
      },
      "source": [
        "# pseudo label\n",
        "cnn_submit[['X', 'Y']] = cnn_submit[['X', 'Y']].clip(-400, 400)\n",
        "wavenet_submit[['X', 'Y']] = wavenet_submit[['X', 'Y']].clip(-400, 400)\n",
        "temp_target = (wavenet_submit/2 + cnn_submit/2).values[:, 1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWhAoJeQf_c5"
      },
      "source": [
        "submit = pd.read_csv('../data/sample_submission.csv')\n",
        "metrics = {}\n",
        "\n",
        "seed_everything(SEED)\n",
        "nf_list = [16, 16, 16]\n",
        "for train_target in range(0,1):\n",
        "    fold_metrics = {}\n",
        "    for k in range(N_FOLDS):\n",
        "        model = set_model(train_target, nf=nf_list[train_target])\n",
        "        train(model, X_train_l[train_target], Y_train, train_target, n_fold=N_FOLDS, fold=k, seed=SEED)\n",
        "        \n",
        "        best_model, fold_metrics[k] = load_best_model(train_target, k)\n",
        "        pred_data_test = best_model.predict(X_test_l[train_target])\n",
        "\n",
        "        if k==0:\n",
        "            if train_target == 0: # x,y 학습\n",
        "                submit.iloc[:,1] = pred_data_test[:,0]/N_FOLDS\n",
        "                submit.iloc[:,2] = pred_data_test[:,1]/N_FOLDS\n",
        "            elif train_target == 1: # m 학습\n",
        "                submit.iloc[:,3] = pred_data_test[:,2]/N_FOLDS\n",
        "            elif train_target == 2: # v 학습\n",
        "                submit.iloc[:,4] = pred_data_test[:,3]/N_FOLDS\n",
        "        else:\n",
        "            if train_target == 0: # x,y 학습\n",
        "                submit.iloc[:,1] += pred_data_test[:,0]/N_FOLDS\n",
        "                submit.iloc[:,2] += pred_data_test[:,1]/N_FOLDS\n",
        "            elif train_target == 1: # m 학습\n",
        "                submit.iloc[:,3] += pred_data_test[:,2]/N_FOLDS\n",
        "            elif train_target == 2: # v 학습\n",
        "                submit.iloc[:,4] += pred_data_test[:,3]/N_FOLDS\n",
        "        print(f'################################################################################ target : {train_target}, fold : {k} ################################################################################')\n",
        "    metrics[train_target] = fold_metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BNtFKiZ2ln6"
      },
      "source": [
        "## 6. 결과 및 결언\n",
        "## Conclusion & Discussion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMEdF_8z2ln7"
      },
      "source": [
        "submit[['X', 'Y']] = submit[['X', 'Y']].clip(-400, 400)\n",
        "submit['M'] = temp_target[:, 2] + 1.8\n",
        "submit['V'] = temp_target[:, 3] + 0.0207"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HQ-FO8Df_c9"
      },
      "source": [
        "submit.to_csv('submission.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}